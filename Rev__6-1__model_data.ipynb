{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tphate # requires Python 3.7 - 3.9\n",
    "\n",
    "import utils__config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My Drive\\\\Residency\\\\Research\\\\Lab - Damisah\\\\Project - Sleep\\\\Revisions'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypno_path = 'Data/S01_Feb02_epochs_10s_hypno.csv'\n",
    "# spikes_path = 'Data/S01_Feb02_epochs_10s_spikes.csv'\n",
    "# swa_path = 'Data/S01_Feb02_epochs_10s_swa.csv'\n",
    "# response_path = 'Data/S01_Feb02_unit_response.csv'\n",
    "# chan_path = 'Data/S01_electrodes.csv'\n",
    "# recording_label = 'S01_Feb02'\n",
    "# output_path = 'Cache/S01_Feb02_model_stack_10s.csv'\n",
    "\n",
    "hypno_path = 'Data/S05_Jul11_epochs_10s_hypno.csv'\n",
    "spikes_path = 'Data/S05_Jul11_epochs_10s_spikes.csv'\n",
    "swa_path = 'Data/S05_Jul11_epochs_10s_swa.csv'\n",
    "response_path = 'Data/S05_Jul11_unit_response.csv'\n",
    "chan_path = 'Data/S05_electrodes.csv'\n",
    "recording_label = 'S05_Jul11'\n",
    "output_path = 'Cache/S05_Jul11_model_stack_10s.csv'\n",
    "\n",
    "# hypno_path = 'Data/S05_Jul12_epochs_10s_hypno.csv'\n",
    "# spikes_path = 'Data/S05_Jul12_epochs_10s_spikes.csv'\n",
    "# swa_path = 'Data/S05_Jul12_epochs_10s_swa.csv'\n",
    "# response_path = 'Data/S05_Jul12_unit_response.csv'\n",
    "# chan_path = 'Data/S05_electrodes.csv'\n",
    "# recording_label = 'S05_Jul12'\n",
    "# output_path = 'Cache/S05_Jul12_model_stack_10s.csv'\n",
    "\n",
    "# hypno_path = 'Data/S05_Jul13_epochs_10s_hypno.csv'\n",
    "# spikes_path = 'Data/S05_Jul13_epochs_10s_spikes.csv'\n",
    "# swa_path = 'Data/S05_Jul13_epochs_10s_swa.csv'\n",
    "# response_path = 'Data/S05_Jul13_unit_response.csv'\n",
    "# chan_path = 'Data/S05_electrodes.csv'\n",
    "# recording_label = 'S05_Jul13'\n",
    "# output_path = 'Cache/S05_Jul13_model_stack_10s.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "tphate_dims = 2\n",
    "tphate_cores = 6\n",
    "np.random.seed(42) # reproducibility for T-PHATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypno_epochs = pd.read_csv(hypno_path)\n",
    "spike_epochs = pd.read_csv(spikes_path)\n",
    "swa_epochs = pd.read_csv(swa_path)\n",
    "response = pd.read_csv(response_path)\n",
    "channels = pd.read_csv(chan_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypnogram dictionary: \n",
    "# (-2) = Unassigned\n",
    "# (-1) = Artifact\n",
    "# (0) = Awake\n",
    "# (1) = N1\n",
    "# (2) = N2\n",
    "# (3) = N3\n",
    "# (4) = REM\n",
    "\n",
    "# Extract and group sleep stages\n",
    "hypno_epochs['NREM'] = np.where(hypno_epochs['stage'].isin([2, 3]), 1, 0)\n",
    "hypno_epochs['WREM'] = np.where(hypno_epochs['stage'].isin([0, 1, 4]), 1, 0)\n",
    "hypno_epochs = hypno_epochs.drop(columns=['stage'])\n",
    "\n",
    "# Format SWA and Channel Info\n",
    "swa_epochs['epoch'] = swa_epochs['epoch'].astype('int64')\n",
    "swa_epochs = swa_epochs[['epoch', 'channel', 'zlog_power', 'sw_ratio']]\n",
    "swa_epochs.columns = ['epoch', 'channel', 'power', 'sw']\n",
    "\n",
    "channels = channels[channels.type == 'macro']\n",
    "channels = channels[['elec_label', 'hemisphere', 'roi_3', 'lobe_1']]\n",
    "channels.columns = ['channel', 'laterality', 'region', 'lobe']\n",
    "\n",
    "# Merge SWA + Channel Info, then calculate global SWA power\n",
    "swa_epochs = swa_epochs.merge(channels, on='channel', how='inner')\n",
    "regional_swa_epochs = swa_epochs.groupby(['epoch', 'laterality', 'lobe'])[['power', 'sw']].mean().reset_index()\n",
    "global_swa_epochs = regional_swa_epochs.groupby(['epoch'])[['power', 'sw']].mean().reset_index()\n",
    "\n",
    "# Format unit response type\n",
    "response = response[['unit', 'response_type']]\n",
    "response.columns = ['unit_id', 'response']\n",
    "response['response'] = response['response'].fillna(\"None\")\n",
    "\n",
    "# Merge spikes with response and generate subsets\n",
    "spikeponse = spike_epochs.merge(response, on='unit_id', how='inner')\n",
    "\n",
    "# Define the subset criteria\n",
    "criteria = [\n",
    "    {\"subset\": \"all_cla\", \"filter\": (spikeponse[\"unit_region\"] == \"CLA\")},\n",
    "    {\"subset\": \"positive_cla\", \"filter\": (spikeponse[\"unit_region\"] == \"CLA\") & (spikeponse[\"response\"] == \"Positive\")},\n",
    "    {\"subset\": \"other_cla\", \"filter\": (spikeponse[\"unit_region\"] == \"CLA\") & (spikeponse[\"response\"] != \"Positive\")},\n",
    "    \n",
    "    {\"subset\": \"all_amy\", \"filter\": (spikeponse[\"unit_region\"] == \"AMY\")},\n",
    "    {\"subset\": \"positive_amy\", \"filter\": (spikeponse[\"unit_region\"] == \"AMY\") & (spikeponse[\"response\"] == \"Positive\")},\n",
    "    {\"subset\": \"other_amy\", \"filter\": (spikeponse[\"unit_region\"] == \"AMY\") & (spikeponse[\"response\"] != \"Positive\")},\n",
    "\n",
    "    {\"subset\": \"all_acc\", \"filter\": (spikeponse[\"unit_region\"] == \"ACC\")},\n",
    "    {\"subset\": \"positive_acc\", \"filter\": (spikeponse[\"unit_region\"] == \"ACC\") & (spikeponse[\"response\"] == \"Positive\")},\n",
    "    {\"subset\": \"other_acc\", \"filter\": (spikeponse[\"unit_region\"] == \"ACC\") & (spikeponse[\"response\"] != \"Positive\")}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firing rate by subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Group mean FR by subset (you will need to adapt the Epoch Stacking code in order to use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit_epochs = pd.DataFrame()\n",
    "\n",
    "# # Loop through each subset criteria\n",
    "# for crit in criteria:\n",
    "\n",
    "#     # Filter the data based on the current criteria\n",
    "#     subset_data = spikeponse[crit[\"filter\"]]\n",
    "    \n",
    "#     # Group by 'epoch' alone to calculate the global mean\n",
    "#     global_mean = subset_data.groupby('epoch')['fr'].mean().reset_index()\n",
    "#     global_mean['subset'] = crit[\"subset\"]  # Add the subset column\n",
    "    \n",
    "#     # Append the result to the 'subsets' DataFrame\n",
    "#     unit_epochs = pd.concat([unit_epochs, global_mean], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: T-PHATE of FR by subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 12 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.22 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.01 seconds.\n",
      "  Calculated graph and diffusion operator in 0.24 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 727\n",
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 18\n",
      "  Calculated optimal t in 7.85 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.84 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 89.88 seconds.\n",
      "Calculated TPHATE in 101.55 seconds.\n",
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 6 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.12 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.02 seconds.\n",
      "  Calculated graph and diffusion operator in 0.14 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 656\n",
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 12\n",
      "  Calculated optimal t in 10.43 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.91 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 93.43 seconds.\n",
      "Calculated TPHATE in 108.98 seconds.\n",
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 6 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.11 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.02 seconds.\n",
      "  Calculated graph and diffusion operator in 0.13 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 834\n",
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 8\n",
      "  Calculated optimal t in 10.43 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.68 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 95.42 seconds.\n",
      "Calculated TPHATE in 110.47 seconds.\n",
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 14 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.23 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.02 seconds.\n",
      "  Calculated graph and diffusion operator in 0.26 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 700\n",
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 17\n",
      "  Calculated optimal t in 10.41 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.93 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 82.23 seconds.\n",
      "Calculated TPHATE in 97.32 seconds.\n",
      "Subset 'positive_amy' resulted in an empty dataframe. Skipping...\n",
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 14 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.23 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.02 seconds.\n",
      "  Calculated graph and diffusion operator in 0.25 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 700\n",
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 17\n",
      "  Calculated optimal t in 13.69 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 1.20 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 93.14 seconds.\n",
      "Calculated TPHATE in 111.86 seconds.\n",
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 4 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.12 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.02 seconds.\n",
      "  Calculated graph and diffusion operator in 0.14 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 356\n",
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 11\n",
      "  Calculated optimal t in 10.94 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 1.15 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 82.82 seconds.\n",
      "Calculated TPHATE in 98.41 seconds.\n",
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 1 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.11 seconds.\n",
      "    Calculating affinities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\layto\\.conda\\envs\\tphate\\lib\\site-packages\\tphate\\graphs.py:303: RuntimeWarning: Detected zero distance between 59300 pairs of samples. Consider removing duplicates to avoid errors in downstream processing.\n",
      "  RuntimeWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Calculated affinities in 0.63 seconds.\n",
      "  Calculated graph and diffusion operator in 0.77 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\layto\\.conda\\envs\\tphate\\lib\\site-packages\\tphate\\graphs.py:461: RuntimeWarning: overflow encountered in power\n",
      "  K.data = np.exp(-1 * np.power(K.data, self.decay))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 5\n",
      "  Calculated optimal t in 8.47 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.63 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 85.90 seconds.\n",
      "Calculated TPHATE in 99.23 seconds.\n",
      "Calculating TPHATE...\n",
      "  Running TPHATE on 3485 observations and 3 variables.\n",
      "  Landmarking not recommended; setting n_landmark to 3485\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.12 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.01 seconds.\n",
      "  Calculated graph and diffusion operator in 0.13 seconds.\n",
      "  Learning the autocorrelation function...\n",
      "  Calculating Autocorr kernel...\n",
      "    Dropoff point: 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\layto\\.conda\\envs\\tphate\\lib\\site-packages\\tphate\\graphs.py:294: RuntimeWarning: Detected zero distance between samples 278 and 2455, 633 and 2059, 684 and 2333, 989 and 1036, 1317 and 2000, 1474 and 2444, 1494 and 1935, 1650 and 2824. Consider removing duplicates to avoid errors in downstream processing.\n",
      "  RuntimeWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining PHATE operator and autocorr operator\n",
      "  Calculating optimal t...\n",
      "    Automatically selected t = 8\n",
      "  Calculated optimal t in 10.25 seconds.\n",
      "  Calculating diffusion potential...\n",
      "  Calculated diffusion potential in 0.69 seconds.\n",
      "  Calculating metric MDS...\n",
      "  Calculated metric MDS in 92.53 seconds.\n",
      "Calculated TPHATE in 106.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store the results\n",
    "unit_epochs = pd.DataFrame()\n",
    "\n",
    "# Function to apply T-PHATE and return a DataFrame with the results\n",
    "def apply_tphate(data, n_components, n_jobs):\n",
    "    tp_model = tphate.TPHATE(n_components=n_components, n_jobs=n_jobs)\n",
    "    transformed_data = tp_model.fit_transform(data)\n",
    "    return pd.DataFrame(transformed_data, columns=[f'tphate_{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Loop through each subset criteria\n",
    "for crit in criteria:\n",
    "    # Filter the data based on the current criteria\n",
    "    subset_data = spikeponse[crit[\"filter\"]]\n",
    "\n",
    "    # Check if the filtered data is empty\n",
    "    if subset_data.empty:\n",
    "        print(f\"Subset '{crit['subset']}' resulted in an empty dataframe. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Count the unique unit_ids in the current subset\n",
    "    unique_unit_num = subset_data['unit_id'].nunique()\n",
    "\n",
    "    # Pivot the data to wide format for T-PHATE\n",
    "    wide_data = subset_data.pivot(index='epoch', columns='unit_id', values='fr').fillna(0)\n",
    "\n",
    "    # Apply T-PHATE\n",
    "    tphate_data = apply_tphate(wide_data.to_numpy(), n_components=tphate_dims, n_jobs = tphate_cores)\n",
    "\n",
    "    # Add back the metadata information\n",
    "    tphate_data['epoch'] = wide_data.index\n",
    "    tphate_data['subset'] = crit[\"subset\"]\n",
    "    tphate_data['unit_num'] = unique_unit_num  # Add the count of unique unit_ids as a new column\n",
    "\n",
    "    # Append the result to the 'unit_epochs' DataFrame\n",
    "    unit_epochs = pd.concat([unit_epochs, tphate_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge FR by subset with hypnogram and LFP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the datasets\n",
    "data = hypno_epochs.merge(unit_epochs, on='epoch', how='inner')\n",
    "data = data.merge(global_swa_epochs, on='epoch', how='inner')\n",
    "\n",
    "# Create 'DREM' (delta-NREM), which is NREM with high delta power\n",
    "data['DREM'] = np.where((data['NREM'] == 1) & (data['power'] > 1), 1, 0)\n",
    "\n",
    "# Final columns for 'data' will be:\n",
    "# 'subset', 'epoch', 'power', 'sw', 'WREM', 'NREM', 'DREM', 'unit_num'\n",
    "# plus either 'fr' (group mean) or 'tphate_1' ... 'tphate_n' (T-PHATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch stacking for lagged regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store DataFrames for each subset\n",
    "all_subsets = []\n",
    "\n",
    "# Loop through each unique subset value\n",
    "for selected_subset in data['subset'].unique():\n",
    "\n",
    "    # Filter the data for the current subset and sort by 'epoch'\n",
    "    subset_data = data[data['subset'] == selected_subset].sort_values('epoch')\n",
    "\n",
    "    # Extract unit_num value (assuming it's the same for all rows within the subset)\n",
    "    unit_num_value = subset_data['unit_num'].iloc[0]\n",
    "\n",
    "    # Identify all tphate columns\n",
    "    tphate_columns = [col for col in subset_data.columns if 'tphate_' in col]\n",
    "\n",
    "    # Initialize a list to store the new rows for the current subset\n",
    "    window_data = []\n",
    "\n",
    "    # Loop through the DataFrame, starting from 'window_size - 1' to have enough previous values\n",
    "    for i in range(window_size - 1, len(subset_data)):\n",
    "        new_row = []\n",
    "\n",
    "        # Loop through each T-PHATE dimension to get the lagged values\n",
    "        for tphate_col in tphate_columns:\n",
    "            \n",
    "            # Use values from current and previous epochs\n",
    "            tphate_lagged = subset_data.iloc[i - window_size + 1:i + 1][tphate_col].values[::-1]\n",
    "            new_row.extend(tphate_lagged)\n",
    "\n",
    "        # Get the 'power', 'NREM', and 'DREM' values for the current epoch\n",
    "        power_value = subset_data.iloc[i]['power']\n",
    "        nrem_value = subset_data.iloc[i]['NREM']\n",
    "        drem_value = subset_data.iloc[i]['DREM']\n",
    "\n",
    "        # Append these values and the current epoch to the new row\n",
    "        new_row.extend([power_value, nrem_value, drem_value, subset_data.iloc[i]['epoch']])\n",
    "\n",
    "        # Add the new row to the window_data list\n",
    "        window_data.append(new_row)\n",
    "\n",
    "    # Column names for the new DataFrame, including the epoch\n",
    "    column_names = [f'{tphate_col}_{window_size - j}' for tphate_col in tphate_columns for j in range(window_size)] + ['power', 'NREM', 'DREM', 'epoch']\n",
    "    \n",
    "    # Convert the list of new rows into a DataFrame for the current subset and add meta-data\n",
    "    subset_stack = pd.DataFrame(window_data, columns=column_names)\n",
    "    subset_stack['subset'] = selected_subset\n",
    "    subset_stack['unit_num'] = unit_num_value\n",
    "    \n",
    "    # Append the DataFrame for the current subset to the list\n",
    "    all_subsets.append(subset_stack)\n",
    "\n",
    "# Concatenate all the subset DataFrames into one\n",
    "data_stack = pd.concat(all_subsets, ignore_index=True)\n",
    "\n",
    "# Assuming 'recording_label' is defined\n",
    "data_stack['recording'] = recording_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stack.to_csv(output_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tphate_1_3</th>\n",
       "      <th>tphate_1_2</th>\n",
       "      <th>tphate_1_1</th>\n",
       "      <th>tphate_2_3</th>\n",
       "      <th>tphate_2_2</th>\n",
       "      <th>tphate_2_1</th>\n",
       "      <th>power</th>\n",
       "      <th>NREM</th>\n",
       "      <th>DREM</th>\n",
       "      <th>epoch</th>\n",
       "      <th>subset</th>\n",
       "      <th>unit_num</th>\n",
       "      <th>recording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.083714</td>\n",
       "      <td>0.084862</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>-0.004305</td>\n",
       "      <td>-0.004292</td>\n",
       "      <td>-0.007667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>all_cla</td>\n",
       "      <td>12</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.082123</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.083714</td>\n",
       "      <td>-0.004328</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>-0.004305</td>\n",
       "      <td>0.093267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>all_cla</td>\n",
       "      <td>12</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081384</td>\n",
       "      <td>0.082123</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>-0.004336</td>\n",
       "      <td>-0.004328</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>all_cla</td>\n",
       "      <td>12</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080590</td>\n",
       "      <td>0.081384</td>\n",
       "      <td>0.082123</td>\n",
       "      <td>-0.004350</td>\n",
       "      <td>-0.004336</td>\n",
       "      <td>-0.004328</td>\n",
       "      <td>0.238600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>all_cla</td>\n",
       "      <td>12</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.079893</td>\n",
       "      <td>0.080590</td>\n",
       "      <td>0.081384</td>\n",
       "      <td>-0.004360</td>\n",
       "      <td>-0.004350</td>\n",
       "      <td>-0.004336</td>\n",
       "      <td>-0.258667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>all_cla</td>\n",
       "      <td>12</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27859</th>\n",
       "      <td>0.080604</td>\n",
       "      <td>0.079014</td>\n",
       "      <td>0.077181</td>\n",
       "      <td>0.044429</td>\n",
       "      <td>0.043885</td>\n",
       "      <td>0.043072</td>\n",
       "      <td>-0.600933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3480</td>\n",
       "      <td>other_acc</td>\n",
       "      <td>3</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27860</th>\n",
       "      <td>0.082536</td>\n",
       "      <td>0.080604</td>\n",
       "      <td>0.079014</td>\n",
       "      <td>0.045249</td>\n",
       "      <td>0.044429</td>\n",
       "      <td>0.043885</td>\n",
       "      <td>-0.578400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3481</td>\n",
       "      <td>other_acc</td>\n",
       "      <td>3</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27861</th>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.082536</td>\n",
       "      <td>0.080604</td>\n",
       "      <td>0.045817</td>\n",
       "      <td>0.045249</td>\n",
       "      <td>0.044429</td>\n",
       "      <td>-0.302933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3482</td>\n",
       "      <td>other_acc</td>\n",
       "      <td>3</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27862</th>\n",
       "      <td>0.086061</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.082536</td>\n",
       "      <td>0.046665</td>\n",
       "      <td>0.045817</td>\n",
       "      <td>0.045249</td>\n",
       "      <td>-0.380667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3483</td>\n",
       "      <td>other_acc</td>\n",
       "      <td>3</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27863</th>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.086061</td>\n",
       "      <td>0.084007</td>\n",
       "      <td>0.047786</td>\n",
       "      <td>0.046665</td>\n",
       "      <td>0.045817</td>\n",
       "      <td>-0.421400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3484</td>\n",
       "      <td>other_acc</td>\n",
       "      <td>3</td>\n",
       "      <td>S05_Jul11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27864 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tphate_1_3  tphate_1_2  tphate_1_1  tphate_2_3  tphate_2_2  tphate_2_1  \\\n",
       "0        0.083020    0.083714    0.084862   -0.004313   -0.004305   -0.004292   \n",
       "1        0.082123    0.083020    0.083714   -0.004328   -0.004313   -0.004305   \n",
       "2        0.081384    0.082123    0.083020   -0.004336   -0.004328   -0.004313   \n",
       "3        0.080590    0.081384    0.082123   -0.004350   -0.004336   -0.004328   \n",
       "4        0.079893    0.080590    0.081384   -0.004360   -0.004350   -0.004336   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "27859    0.080604    0.079014    0.077181    0.044429    0.043885    0.043072   \n",
       "27860    0.082536    0.080604    0.079014    0.045249    0.044429    0.043885   \n",
       "27861    0.084007    0.082536    0.080604    0.045817    0.045249    0.044429   \n",
       "27862    0.086061    0.084007    0.082536    0.046665    0.045817    0.045249   \n",
       "27863    0.088900    0.086061    0.084007    0.047786    0.046665    0.045817   \n",
       "\n",
       "          power  NREM  DREM  epoch     subset  unit_num  recording  \n",
       "0     -0.007667     0     0      2    all_cla        12  S05_Jul11  \n",
       "1      0.093267     0     0      3    all_cla        12  S05_Jul11  \n",
       "2      0.010867     0     0      4    all_cla        12  S05_Jul11  \n",
       "3      0.238600     0     0      5    all_cla        12  S05_Jul11  \n",
       "4     -0.258667     0     0      6    all_cla        12  S05_Jul11  \n",
       "...         ...   ...   ...    ...        ...       ...        ...  \n",
       "27859 -0.600933     0     0   3480  other_acc         3  S05_Jul11  \n",
       "27860 -0.578400     0     0   3481  other_acc         3  S05_Jul11  \n",
       "27861 -0.302933     0     0   3482  other_acc         3  S05_Jul11  \n",
       "27862 -0.380667     0     0   3483  other_acc         3  S05_Jul11  \n",
       "27863 -0.421400     0     0   3484  other_acc         3  S05_Jul11  \n",
       "\n",
       "[27864 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import neo\n",
    "import quantities as pq\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from elephant.conversion import BinnedSpikeTrain as elephant_bst\n",
    "from elephant.spike_train_correlation import cross_correlation_histogram as elephant_cch\n",
    "import elephant.spike_train_surrogates as sgt\n",
    "\n",
    "from itertools import combinations, product\n",
    "\n",
    "import utils__config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\Layton\\\\Sleep_051324'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cross-correlation pipeline is implemented via what I call \"double permutation\": One permutation to convert the raw cross-correlograms into z-scored (or t valued) cross-correlograms, and one permutation to convert TFCE-valued cross-correlograms into p-valued cross-correlograms. My steps are below:\n",
    "\n",
    "- First, for each unit pair (two spike trains), I compute the \"original\" cross-correlogram.\n",
    "\n",
    "- Second, to pair with the above \"original\" cross-correlogram (independently for each unit pair), I compute a surrogate cross-correlogram by starting with the same data, but every spike time in the second spike train has been jittered randomly (you only need to jitter one, not both). This is repeated 1000 times.\n",
    "\n",
    "- Third, I can then transform the value of each bin in the \"original\" cross-correlogram into a z-score by using the mean/SD from the corresponding bins of the surrogate cross-correlograms.\n",
    "\n",
    "- Fourth, I perform TFCE for each unit pair's cross-correlogram independently, generating a single scalar TFCE value for each bin (thus, a TFCE-valued cross-correlogram). Save only the maximum TFCE bin value from each original cross-correlogram.\n",
    "\n",
    "- Fifth, to pair with the above TFCE-valued cross-correlogram (independently for each unit pair), I generate a surrogate TFCE cross-correlogram by shuffling the bins randomly and calculating TFCE values for each bin. I save only the maximum TFCE value from the whole cross-correlogram (the highest bin value amongst all bins). This is repeated 1000 times (in order to generate a distribution of maximum TFCE values).\n",
    "\n",
    "- Sixth, for each original cross-correlogram, its p-value is the percent of the maximum surrogate TFCE values that are higher than its TFCE value. Thus, it is significant if its max TFCE value is larger than the 95% of the surrogate maximum TFCE values.*\n",
    "\n",
    "- Seventh, each bin's p-value will have FDR correction applied based on how many unit pairs underwent this pipeline.\n",
    "\n",
    "*If you saved all TFCE bin values from the original cross-correlogram in Step 4, you could individually compare each original TFCE bin value to the surrogate max TFCE value distribution in Step 6. This would allow you to highlight individual bins/clusters that were significant rather than just whole cross-correlograms. However, I believe it would increase the number of comparisons and force you to calculate the FDR correction using (unit pairs x bins per correlogram) rather than just (unit pairs).\n",
    "\n",
    "Finally, we can visualize the results with two histograms, both stratified by region pairing (e.g. CLA-ACC or CLA-AMY):\n",
    "\n",
    "- In the first histogram, we plot the maximum TFCE bin value of the original histogram along the x-axis and the count of unit pairs along the y-axis. Coloring indicates significance. This demonstrates the total number of unit pair interactions that were analyzed with significant pairs highlighted.\n",
    "\n",
    "- In the second histogram, for significant unit pairs, we plot the bin number (i.e. time lag) of the maximum TFCE bin value of the original histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'Data/S01_Feb02_spike_times.csv'\n",
    "hypno_path = 'Data/S01_Feb02_hypnogram.csv' \n",
    "output_path_zscore = 'Cache/S01_Feb02_zcorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "output_path_original = 'Cache/S01_Feb02_ocorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "recording_length = 2.01 # hours\n",
    "\n",
    "# input_path = 'Data/S05_Jul11_spike_times.csv'\n",
    "# hypno_path = 'Data/S05_Jul11_hypnogram.csv' \n",
    "# output_path_zscore = 'Cache/S05_Jul11_zcorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "# output_path_original = 'Cache/S05_Jul11_ocorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "# recording_length = 9.69 # hours\n",
    "\n",
    "# input_path = 'Data/S05_Jul12_spike_times.csv'\n",
    "# hypno_path = 'Data/S05_Jul12_hypnogram.csv' \n",
    "# output_path_zscore = 'Cache/S05_Jul12_zcorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "# output_path_original = 'Cache/S05_Jul12_ocorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "# recording_length = 10.55 # hours\n",
    "\n",
    "# input_path = 'Data/S05_Jul13_spike_times.csv'\n",
    "# hypno_path = 'Data/S05_Jul13_hypnogram.csv' \n",
    "# output_path_zscore = 'Cache/S05_Jul13_zcorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "# output_path_original = 'Cache/S05_Jul13_ocorrelogram_b5_w10_n1000_nrem_allpairs.csv'\n",
    "# recording_length = 10.4 # hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrem_spikes_only = True\n",
    "all_region_pairs = True\n",
    "surrogate_num = 1000\n",
    "\n",
    "hypno_sampling_freq = 256\n",
    "bin_size = 5 * pq.ms\n",
    "cc_window = [-10, 10] # in number of bins\n",
    "dither = 20 * pq.ms\n",
    "n_cores = -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format spike times into Neo SpikeTrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Pandas dataframe\n",
    "spike_times = pd.read_csv(input_path)\n",
    "spike_times = spike_times[['unit_laterality', 'unit_region', 'unit_id', 'seconds']]\n",
    "\n",
    "# Set recording start and stop lengths in seconds\n",
    "t_start = 0\n",
    "t_stop = recording_length * 3600\n",
    "\n",
    "# Hypnogram dictionary: \n",
    "# (-2) = Unassigned\n",
    "# (-1) = Artifact\n",
    "# (0) = Awake\n",
    "# (1) = N1\n",
    "# (2) = N2\n",
    "# (3) = N3\n",
    "# (4) = REM\n",
    "\n",
    "# Load hypnogram and merge\n",
    "hypno = pd.read_csv(hypno_path, header = None)\n",
    "hypno = hypno.reset_index()\n",
    "hypno.columns = ['sample', 'stage']\n",
    "hypno['time'] = hypno['sample'] / hypno_sampling_freq\n",
    "\n",
    "# Extract and group sleep stages\n",
    "hypno['stage'] = np.where(hypno['stage'].isin([2, 3]), 'NREM', 'WREM')\n",
    "\n",
    "# Merge spikes with nearest sleep stage\n",
    "spike_times = pd.merge_asof(spike_times.sort_values('seconds'), hypno.sort_values('time'),\n",
    "                            left_on = 'seconds', right_on = 'time', direction = 'nearest')\n",
    "\n",
    "spike_times.drop(columns=['sample', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep NREM spikes only if nrem_spikes_only == True\n",
    "if nrem_spikes_only:\n",
    "    spike_times = spike_times[spike_times['stage'] == 'NREM']\n",
    "\n",
    "# Create a mapping from unit_id to unit_region\n",
    "unit_region_map = spike_times.drop_duplicates(subset=['unit_id']).set_index('unit_id')['unit_region'].to_dict()\n",
    "\n",
    "# Convert the DataFrame into a list of Neo SpikeTrains with annotations\n",
    "spike_trains = []\n",
    "for unit_id in spike_times['unit_id'].unique():\n",
    "    # Subset the DataFrame to get the spike times for the current unit\n",
    "    unit_times = spike_times[spike_times['unit_id'] == unit_id]['seconds'].values\n",
    "    \n",
    "    # Create the SpikeTrain\n",
    "    st = neo.SpikeTrain(unit_times * pq.s, t_start=t_start * pq.s, t_stop=t_stop * pq.s)\n",
    "    \n",
    "    # Annotate with unit_id and unit_region\n",
    "    st.annotate(unit_id=unit_id, unit_region=unit_region_map[unit_id])\n",
    "    \n",
    "    spike_trains.append(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute cross-correlation histograms\n",
    "def compute_cross_correlation(spike_train_1, spike_train_2, bin_size):\n",
    "    \n",
    "    binned_st1 = elephant_bst(spike_train_1, bin_size=bin_size)\n",
    "    binned_st2 = elephant_bst(spike_train_2, bin_size=bin_size)\n",
    "    \n",
    "    cc_hist, lags = elephant_cch(binned_st1, binned_st2, window=cc_window, border_correction=False, binary=False, kernel=None)\n",
    "\n",
    "    return cc_hist.magnitude.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_annotate_correlations(spike_train_1, spike_train_2, bin_size, surrogate_num, dither):\n",
    "    # Generate surrogate spike trains for spike_train_2\n",
    "    surrogate_spike_trains = sgt.surrogates(spiketrain=spike_train_2, n_surrogates=surrogate_num, \n",
    "                                            method='dither_spikes', dt=dither)\n",
    "    \n",
    "    # Compute cross-correlation for the original pair\n",
    "    original_hist_values = compute_cross_correlation(spike_train_1, spike_train_2, bin_size)\n",
    "\n",
    "    # Initialize array to store surrogate cross-correlations\n",
    "    surrogate_values = np.zeros((surrogate_num, len(original_hist_values)))\n",
    "\n",
    "    # Compute cross-correlation for each surrogate\n",
    "    for i, surrogate in enumerate(surrogate_spike_trains):\n",
    "        hist_values = compute_cross_correlation(spike_train_1, surrogate, bin_size)\n",
    "        surrogate_values[i, :] = hist_values\n",
    "    \n",
    "    # Calculate mean and std of surrogate cross-correlations for each bin\n",
    "    surrogate_mean = np.mean(surrogate_values, axis=0)\n",
    "    surrogate_std = np.std(surrogate_values, axis=0)\n",
    "\n",
    "    # Calculate z-score for the original cross-correlation for each bin\n",
    "    z_scores = (original_hist_values - surrogate_mean) / surrogate_std\n",
    "\n",
    "    # Save the unit ID's for metadata\n",
    "    unit_id_1 = spike_train_1.annotations['unit_id']\n",
    "    unit_id_2 = spike_train_2.annotations['unit_id']\n",
    "\n",
    "    z_scored_result = {\"unit_1\": unit_id_1, \"unit_2\": unit_id_2}\n",
    "    original_result = {\"unit_1\": unit_id_1, \"unit_2\": unit_id_2}\n",
    "\n",
    "    # Name lag columns by ordinality\n",
    "    for ordinal, z_score in enumerate(z_scores, start=1):\n",
    "        z_scored_result[f\"lag_{ordinal}\"] = z_score\n",
    "\n",
    "    for ordinal, value in enumerate(original_hist_values, start=1):\n",
    "        original_result[f\"lag_{ordinal}\"] = value\n",
    "    \n",
    "    return z_scored_result, original_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cross-correlation with original and surrogate spike trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_region_pairs:\n",
    "    # Prepare the list of all unique combinations of spike trains\n",
    "    spike_train_pairs = list(combinations(spike_trains, 2))\n",
    "    \n",
    "else:\n",
    "    # Filter spike_trains based on unit_region\n",
    "    cla_spike_trains = [st for st in spike_trains if st.annotations['unit_region'] == 'CLA']\n",
    "    amy_acc_spike_trains = [st for st in spike_trains if st.annotations['unit_region'] in ['AMY', 'ACC']]\n",
    "\n",
    "    # Prepare the list of all unique combinations where one unit is from CLA and the other from AMY or ACC\n",
    "    spike_train_pairs = list(product(cla_spike_trains, amy_acc_spike_trains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 290/666 [7:23:49<10:30:36, 100.63s/it]"
     ]
    }
   ],
   "source": [
    "# Execute the processing in parallel and collect both z-scored results and original correlogram results\n",
    "results = Parallel(n_jobs=n_cores)(\n",
    "    delayed(compute_and_annotate_correlations)(spike_train_1, spike_train_2, bin_size, surrogate_num, dither)\n",
    "    for spike_train_1, spike_train_2 in tqdm(spike_train_pairs)\n",
    ")\n",
    "\n",
    "# Initialize lists to store the z-scored correlograms and original correlograms\n",
    "zscored_correlograms = []\n",
    "original_correlograms = []\n",
    "\n",
    "# Iterate through the results to separate the z-scored and original correlograms\n",
    "for z_scored_result, original_hist_values in results:\n",
    "    zscored_correlograms.append(z_scored_result)\n",
    "    original_correlograms.append(original_hist_values)\n",
    "\n",
    "# Convert the lists of results to pandas DataFrames\n",
    "zrellogram = pd.DataFrame(zscored_correlograms)\n",
    "orellogram = pd.DataFrame(original_correlograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zrellogram.to_csv(output_path_zscore, index = False)\n",
    "orellogram.to_csv(output_path_original, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

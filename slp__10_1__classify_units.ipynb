{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import utils__config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My Drive\\\\Residency\\\\Research\\\\Lab - Damisah\\\\Project - Sleep\\\\Revisions'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = [\n",
    "    {\n",
    "        'recording_id': 'Feb02',\n",
    "        'recording_length': 2,\n",
    "        'spike_times_path': 'Data/S01_Feb02_spike_times.csv',\n",
    "        'spike_forms_path': 'Data/S01_Feb02_spike_waveforms.csv'\n",
    "    },\n",
    "    {\n",
    "        'recording_id': 'Jul11',\n",
    "        'recording_length': 9.68,\n",
    "        'spike_times_path': 'Data/S05_Jul11_spike_times.csv',\n",
    "        'spike_forms_path': 'Data/S05_Jul11_spike_waveforms.csv'\n",
    "    },\n",
    "    {\n",
    "        'recording_id': 'Jul12',\n",
    "        'recording_length': 10.55,\n",
    "        'spike_times_path': 'Data/S05_Jul12_spike_times.csv',\n",
    "        'spike_forms_path': 'Data/S05_Jul12_spike_waveforms.csv'\n",
    "    },\n",
    "    {\n",
    "        'recording_id': 'Jul13',\n",
    "        'recording_length': 10.40,\n",
    "        'spike_times_path': 'Data/S05_Jul13_spike_times.csv',\n",
    "        'spike_forms_path': 'Data/S05_Jul13_spike_waveforms.csv'\n",
    "    }\n",
    "]\n",
    "\n",
    "output_path = 'Data/cell_type_metrics.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to store dataframes for waveforms and times\n",
    "waveforms_dfs = []\n",
    "times_dfs = []\n",
    "\n",
    "# Process each recording\n",
    "for recording in recordings:\n",
    "    # Load and process waveforms\n",
    "    waveforms = pd.read_csv(recording['spike_forms_path'])\n",
    "    waveforms = waveforms[['unit_id', 'time_point', 'amplitude']]\n",
    "    waveforms.columns = ['unit_id_old', 'time_point', 'amplitude']\n",
    "    waveforms['unit_id'] = waveforms['unit_id_old'].astype(str) + '_' + recording['recording_id']\n",
    "    waveforms_dfs.append(waveforms)\n",
    "    \n",
    "    # Load and process times\n",
    "    times = pd.read_csv(recording['spike_times_path'])\n",
    "    times = times[['unit_id', 'seconds']]\n",
    "    times.columns = ['unit_id_old', 'time']\n",
    "    times['unit_id'] = times['unit_id_old'].astype(str) + '_' + recording['recording_id']\n",
    "    times['recording_id'] = recording['recording_id']\n",
    "    times_dfs.append(times)\n",
    "\n",
    "# Concatenate all dataframes for waveforms and times\n",
    "waveforms = pd.concat(waveforms_dfs, ignore_index=True)\n",
    "times = pd.concat(times_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unit_id and count the number of spikes for each unit_id\n",
    "spike_counts = times.groupby('unit_id').size()\n",
    "\n",
    "# Extract the recording_id from the unit_id and map it to its corresponding recording length in seconds\n",
    "recording_lengths = {rec['recording_id']: rec['recording_length'] * 3600 for rec in recordings}  # convert hours to seconds\n",
    "times['recording_length_seconds'] = times['recording_id'].map(recording_lengths)\n",
    "\n",
    "# Ensure that each unit_id has the same recording length (this should be the case)\n",
    "recording_lengths_by_unit = times.groupby('unit_id')['recording_length_seconds'].first()\n",
    "\n",
    "# Calculate the average firing rate for each unit_id\n",
    "firing_rates = spike_counts / recording_lengths_by_unit\n",
    "\n",
    "# Convert the Series into a DataFrame\n",
    "firing_rates = firing_rates.reset_index()\n",
    "firing_rates.columns = ['unit_id', 'firing_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trough-to-peak time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trough_to_peak_time(waveforms_df, sampling_rate=30000):\n",
    "    \"\"\"\n",
    "    Calculate the trough-to-peak time for each neuron's average waveform (or will assign NaN if unit is positive-spiking).\n",
    "\n",
    "    Parameters:\n",
    "    - waveforms_df: A pandas DataFrame with columns 'unit_id', 'time_point', and 'amplitude'.\n",
    "    - sampling_rate: The sampling rate in Hz. Default is 30000.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with columns 'unit_id' and 'trough_to_peak_time'.\n",
    "    \"\"\"\n",
    "    # Reshape the data to wide format\n",
    "    waveforms_wide = waveforms_df.pivot(index='time_point', columns='unit_id', values='amplitude')\n",
    "    \n",
    "    unit_ids = []\n",
    "    times_to_peak = []\n",
    "\n",
    "    # Loop through each unit\n",
    "    for unit in waveforms_wide.columns:\n",
    "        # Check if the unit is positive-spiking\n",
    "        if \"pos\" in unit:\n",
    "            unit_ids.append(unit)\n",
    "            times_to_peak.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # If the unit is negative-spiking, compute the trough-to-peak time\n",
    "        waveform = waveforms_wide[unit].values\n",
    "        trough_idx = np.argmin(waveform)\n",
    "        peak_idx = trough_idx + np.argmax(waveform[trough_idx:])\n",
    "        samples_to_peak = peak_idx - trough_idx\n",
    "        time_to_peak = (samples_to_peak / sampling_rate) * 1000\n",
    "\n",
    "        unit_ids.append(unit)\n",
    "        times_to_peak.append(time_to_peak)\n",
    "\n",
    "    # Create a DataFrame for the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'unit_id': unit_ids,\n",
    "        'trough_to_peak': times_to_peak\n",
    "    })\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp_times = trough_to_peak_time(waveforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-Width Half Maximum (FWHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_width_fwhm(waveforms_df, sampling_rate=30000):\n",
    "    \"\"\"\n",
    "    Calculate the Full Width at Half Maximum (FWHM) for each neuron's average waveform,\n",
    "    distinguishing between positive and negative spiking units based on 'unit_id'.\n",
    "\n",
    "    Parameters:\n",
    "    - waveforms_df: A pandas DataFrame with columns 'unit_id', 'time_point', and 'amplitude'.\n",
    "    - sampling_rate: The sampling rate in Hz. Default is 30000.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with columns 'unit_id' and 'fwhm'.\n",
    "    \"\"\"\n",
    "    \n",
    "    waveforms_wide = waveforms_df.pivot(index='time_point', columns='unit_id', values='amplitude')\n",
    "    \n",
    "    unit_ids = []\n",
    "    fwhm_values = []\n",
    "\n",
    "    for unit in waveforms_wide.columns:\n",
    "        waveform = waveforms_wide[unit].values\n",
    "\n",
    "        if \"_neg_\" in unit:  # Negative-spiking unit\n",
    "\n",
    "            half_amplitude = np.min(waveform) / 2\n",
    "            reference_idx = np.argmin(waveform)\n",
    "\n",
    "        elif \"_pos_\" in unit:  # Positive-spiking unit\n",
    "\n",
    "            half_amplitude = np.max(waveform) / 2\n",
    "            reference_idx = np.argmax(waveform)\n",
    "\n",
    "        # Find indices where waveform crosses the half amplitude level\n",
    "        cross_points = np.where(np.diff(np.sign(waveform - half_amplitude)))[0]\n",
    "\n",
    "        # Find the closest points to the reference index (trough or peak)\n",
    "        before_idx = cross_points[cross_points < reference_idx][-1] if len(cross_points[cross_points < reference_idx]) > 0 else reference_idx\n",
    "        after_idx = cross_points[cross_points > reference_idx][0] if len(cross_points[cross_points > reference_idx]) > 0 else reference_idx\n",
    "        \n",
    "        # Calculate FWHM\n",
    "        samples_width = after_idx - before_idx\n",
    "        fwhm_time = (samples_width / sampling_rate) * 1000  # Convert to milliseconds\n",
    "\n",
    "        unit_ids.append(unit)\n",
    "        fwhm_values.append(fwhm_time)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'unit_id': unit_ids,\n",
    "        'fwhm': fwhm_values\n",
    "    })\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwhm_times = spike_width_fwhm(waveforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burst Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ISI threshold for bursts (10 ms)\n",
    "threshold = 0.01\n",
    "\n",
    "def calculate_burst_index(group):\n",
    "    # Calculate ISIs\n",
    "    isis = group['time'].diff().dropna()\n",
    "    \n",
    "    # Identify spikes that are part of a burst\n",
    "    burst_spikes = isis[isis < threshold].count() + 1  # + 1 to account for the first spike in each burst\n",
    "    \n",
    "    # Calculate Burst Index\n",
    "    bi = burst_spikes / len(group)\n",
    "    \n",
    "    return bi\n",
    "\n",
    "# Calculate Burst Index for each unit\n",
    "burst_indices = times.groupby('unit_id').apply(calculate_burst_index).reset_index()\n",
    "burst_indices.columns = ['unit_id', 'burst_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge metrics and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>firing_rate</th>\n",
       "      <th>trough_to_peak</th>\n",
       "      <th>burst_index</th>\n",
       "      <th>fwhm</th>\n",
       "      <th>log_firing_rate</th>\n",
       "      <th>unit_id_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01_Ch195_neg_Unit3_Feb02</td>\n",
       "      <td>2.494861</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.082225</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.318960</td>\n",
       "      <td>S01_Ch195_neg_Unit3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S01_Ch195_pos_Unit2_Feb02</td>\n",
       "      <td>2.695000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063080</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1.430285</td>\n",
       "      <td>S01_Ch195_pos_Unit2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S01_Ch196_neg_Unit1_Feb02</td>\n",
       "      <td>1.947639</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.038793</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.961726</td>\n",
       "      <td>S01_Ch196_neg_Unit1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S01_Ch196_neg_Unit3_Feb02</td>\n",
       "      <td>1.253750</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>0.039548</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.326250</td>\n",
       "      <td>S01_Ch196_neg_Unit3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S01_Ch196_neg_Unit4_Feb02</td>\n",
       "      <td>1.324167</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.062828</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.405085</td>\n",
       "      <td>S01_Ch196_neg_Unit4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>S05_Ch239_neg_Unit3_Jul11</td>\n",
       "      <td>1.039199</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.103413</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.055472</td>\n",
       "      <td>S05_Ch239_neg_Unit3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>S05_Ch240_neg_Unit1_Jul11</td>\n",
       "      <td>10.708907</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.164058</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>3.420739</td>\n",
       "      <td>S05_Ch240_neg_Unit1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>S05_Ch240_neg_Unit2_Jul12</td>\n",
       "      <td>4.995735</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>0.097055</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.320697</td>\n",
       "      <td>S05_Ch240_neg_Unit2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>S05_Ch240_neg_Unit2_Jul13</td>\n",
       "      <td>9.179140</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.137654</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.198359</td>\n",
       "      <td>S05_Ch240_neg_Unit2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>S05_Ch240_neg_Unit3_Jul13</td>\n",
       "      <td>1.488275</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.033578</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.573641</td>\n",
       "      <td>S05_Ch240_neg_Unit3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       unit_id  firing_rate  trough_to_peak  burst_index  \\\n",
       "0    S01_Ch195_neg_Unit3_Feb02     2.494861        0.566667     0.082225   \n",
       "1    S01_Ch195_pos_Unit2_Feb02     2.695000             NaN     0.063080   \n",
       "2    S01_Ch196_neg_Unit1_Feb02     1.947639        0.533333     0.038793   \n",
       "3    S01_Ch196_neg_Unit3_Feb02     1.253750        1.433333     0.039548   \n",
       "4    S01_Ch196_neg_Unit4_Feb02     1.324167        0.500000     0.062828   \n",
       "..                         ...          ...             ...          ...   \n",
       "117  S05_Ch239_neg_Unit3_Jul11     1.039199        1.100000     0.103413   \n",
       "118  S05_Ch240_neg_Unit1_Jul11    10.708907        1.200000     0.164058   \n",
       "119  S05_Ch240_neg_Unit2_Jul12     4.995735        1.066667     0.097055   \n",
       "120  S05_Ch240_neg_Unit2_Jul13     9.179140        1.133333     0.137654   \n",
       "121  S05_Ch240_neg_Unit3_Jul13     1.488275        1.166667     0.033578   \n",
       "\n",
       "         fwhm  log_firing_rate          unit_id_old  \n",
       "0    0.166667         1.318960  S01_Ch195_neg_Unit3  \n",
       "1    0.433333         1.430285  S01_Ch195_pos_Unit2  \n",
       "2    0.166667         0.961726  S01_Ch196_neg_Unit1  \n",
       "3    0.366667         0.326250  S01_Ch196_neg_Unit3  \n",
       "4    0.166667         0.405085  S01_Ch196_neg_Unit4  \n",
       "..        ...              ...                  ...  \n",
       "117  0.333333         0.055472  S05_Ch239_neg_Unit3  \n",
       "118  0.300000         3.420739  S05_Ch240_neg_Unit1  \n",
       "119  0.200000         2.320697  S05_Ch240_neg_Unit2  \n",
       "120  0.200000         3.198359  S05_Ch240_neg_Unit2  \n",
       "121  0.300000         0.573641  S05_Ch240_neg_Unit3  \n",
       "\n",
       "[122 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = firing_rates.merge(ttp_times, on='unit_id').merge(burst_indices, on='unit_id').merge(fwhm_times, on='unit_id')\n",
    "merged_df['log_firing_rate'] = np.log2(merged_df['firing_rate'])\n",
    "\n",
    "id_frame = waveforms[['unit_id', 'unit_id_old']].drop_duplicates()\n",
    "final_df = merged_df.merge(id_frame, on='unit_id', how='left')\n",
    "\n",
    "final_df.to_csv(output_path)\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

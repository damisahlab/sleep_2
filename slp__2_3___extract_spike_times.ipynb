{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "import hdf5storage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils__config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\My Drive\\\\Residency\\\\Research\\\\Lab - Damisah\\\\Project - Sleep'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will take the output directory from Combinato and extract spike times and associated meta-data from the various H5 files contained in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = 'Cache/Subject01/Micro_Feb02'\n",
    "# dict_dir = 'Data/Subject01/S01_dictionary.xlsx'\n",
    "# metric_out_path = 'Cache/Subject01/S01_spike_metrics.csv'\n",
    "# spike_out_path = 'Cache/Subject01/S01_spikes.csv'\n",
    "# recording_length = 2 # length of recording in hours\n",
    "\n",
    "root_dir = 'Cache/Subject02/Apr26/S02_Apr26_Combinato_Sorted'\n",
    "dict_dir = 'Data/Subject02/S02_dictionary.xlsx'\n",
    "metric_out_path = 'Cache/Subject02/Apr26/S02_spike_metrics.csv'\n",
    "spike_out_path = 'Cache/Subject02/Apr26/S02_spikes.csv'\n",
    "recording_length = 5.63\n",
    "\n",
    "# root_dir = 'Cache/Subject02/Apr27/Micro_Apr27'\n",
    "# dict_dir = 'Data/Subject02/S02_dictionary.xlsx'\n",
    "# metric_out_path = 'Cache/Subject02/Apr27/S02_spike_metrics.csv'\n",
    "# spike_out_path = 'Cache/Subject02/Apr27/S02_spikes.csv'\n",
    "# recording_length = 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S02, Apr26: 01:59:20:000 UTC - 07:37:20:000 UTC = 5hr 38min = 5.63 hours\n",
    "# S02, Apr27: 03:37:50:000 UTC - 08:13:50:000 UTC = 4hr 36min = 4.60 hours\n",
    "\n",
    "selected_regions = ['CLA', 'AMY', 'ACC']\n",
    "MAT_version = '7.3' # MAT file version (SciPy reads < 7.3, hdf5storage reads >= 7.3)\n",
    "min_firing_rate = 0 # minimum firing rate in Hz\n",
    "min_spike_count = min_firing_rate * recording_length * 60 * 60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_dict = pd.read_excel(dict_dir)\n",
    "micro_dict = micro_dict[['number', 'laterality', 'region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [02:31<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "for channel in tqdm(os.listdir(root_dir)):\n",
    "\n",
    "    if MAT_version == '7.3':\n",
    "        raw_data = hdf5storage.loadmat(os.path.join(root_dir, channel))\n",
    "    else:\n",
    "        raw_data = io.loadmat(os.path.join(root_dir, channel))\n",
    "\n",
    "    chan_data = pd.DataFrame()\n",
    "\n",
    "    # Extract unit type and spike times\n",
    "    for unit in np.arange(0, len(raw_data['sp_types'])):\n",
    "\n",
    "        # Skip this unit if it has no spikes\n",
    "        if raw_data['sp_times'][unit][0].shape[1] == 0:\n",
    "        \n",
    "            continue\n",
    "        \n",
    "        # Extract unit type & spike times\n",
    "        unit_type = raw_data['sp_types'][unit][0]\n",
    "        unit_times = raw_data['sp_times'][unit][0][:, 0] # .squeeze() won't work if you only have one spike time\n",
    "\n",
    "        # Format data\n",
    "        unit_data = pd.DataFrame(unit_times, columns = ['ms'])\n",
    "        unit_data['unit_type'] = unit_type\n",
    "        unit_data['unit_num'] = unit + 1\n",
    "\n",
    "        chan_data = pd.concat([chan_data, unit_data])\n",
    "    \n",
    "    # Extract channel meta-data\n",
    "    chan_data['subject'] = channel.split('_')[0]\n",
    "\n",
    "    chan_data['channel'] = channel.split('_')[1]\n",
    "    chan_data['channel'] = chan_data['channel'].str.split('l', expand = True)[1]\n",
    "    chan_data['channel'] = chan_data['channel'].astype('int64') # to merge with micro_dict\n",
    "\n",
    "    chan_data['sign'] = channel.split('_')[2]\n",
    "    chan_data['sign'] = chan_data['sign'].str.split('.', expand = True)[0]\n",
    "\n",
    "    data = pd.concat([data, chan_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with dictionary meta-data\n",
    "data = data.merge(micro_dict, left_on = 'channel', right_on = 'number')\n",
    "\n",
    "# Convert from milliseconds to seconds\n",
    "data['seconds'] = data['ms'] / 1000 \n",
    "\n",
    "# Account for the offset in unit number between Combinato and MATLAB\n",
    "# so that you can compare units between Combinato GUI and your analysis (optional)\n",
    "data['unit_num'] = data['unit_num'] - 1\n",
    "\n",
    "# Create a unique unit ID\n",
    "data['unit_id'] = data['subject'] + '_Ch' + data['channel'].astype('str') + '_' + data['sign'] + '_Unit' + data['unit_num'].astype('str')\n",
    "\n",
    "# Rename laterality/region columns to specify that they apply to the unit\n",
    "data.rename(columns = {'laterality' : 'unit_laterality', 'region' : 'unit_region'}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove artifactual units\n",
    "# (artifact = -1 | unassigned = 0 | MUA = 1 | SUA = 2)\n",
    "#data = data[data['unit_type'] == 2] # SUA only\n",
    "data = data[(data['unit_type'] != -1) & (data['unit_type'] != 0)] # MUA + SUA\n",
    "\n",
    "# Keep only units from certain regions\n",
    "data = data[data['unit_region'].isin(selected_regions)]\n",
    "\n",
    "# Calculate ISI's between each spike by unit and mark those < 3 ms\n",
    "data['diff'] = data.groupby(['unit_id'])['seconds'].diff()\n",
    "data['short_isi'] = np.where(data['diff'] < 0.003, 1, 0)\n",
    "\n",
    "# Calculate total spike count and number of ISI's < 3 ms\n",
    "spike_metrics = data.groupby(['unit_id']).agg({'seconds' : ['count'], 'short_isi': ['sum']}).reset_index()\n",
    "spike_metrics.columns = spike_metrics.columns.droplevel()\n",
    "spike_metrics.columns = ['unit_id', 'num_count', 'num_isi']\n",
    "\n",
    "# Mark units with ISI violations < 3 ms in more than 5% of spikes\n",
    "spike_metrics['perc_isi_violations'] = spike_metrics['num_isi'] / spike_metrics['num_count']\n",
    "spike_metrics['isi_violator'] = np.where(spike_metrics['perc_isi_violations'] >= 0.05, 1, 0)\n",
    "\n",
    "# Remove units that violate ISI or have too few spikes\n",
    "spike_metrics = spike_metrics[(spike_metrics['isi_violator'] == 0) & (spike_metrics['num_count'] >= min_spike_count)]\n",
    "spike_metrics.to_csv(metric_out_path, index = False)\n",
    "\n",
    "data = data[data['unit_id'].isin(spike_metrics['unit_id'])]\n",
    "\n",
    "# Export\n",
    "data.drop(['number', 'ms'], axis = 1, inplace = True)\n",
    "data.to_csv(spike_out_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bceaa3bdda3825794b37c15d2000316b6f4a45a3d4f5e14660beed4f1d5f7638"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

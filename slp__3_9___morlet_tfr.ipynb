{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import mne\n",
    "from mne.time_frequency import tfr_array_morlet\n",
    "from scipy.stats import zscore\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils__helpers_macro import hilbert_powerphase, hilbert_envelope\n",
    "import utils__config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\My Drive\\\\Residency\\\\Research\\\\Lab - Damisah\\\\Project - Sleep'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fif_path = 'Cache/Subject01/S01_Feb02_256hz.fif'\n",
    "tfr_path = 'Cache/Subject01/S01_tfr_30s_epochs.csv'\n",
    "\n",
    "# fif_path = 'Cache/Subject02/Apr26/S02_Apr26_256hz.fif'\n",
    "# tfr_path = 'Cache/Subject02/Apr26/S02_tfr_30s_epochs.csv'\n",
    "\n",
    "# fif_path = 'Cache/Subject02/Apr27/S02_Apr27_256hz.fif'\n",
    "# tfr_path = 'Cache/Subject02/Apr27/S02_tfr_30s_epochs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_freq = 256\n",
    "resample_frequency = 128 # frequency to resample to prior to Morlet (reduces memory usage)\n",
    "tfr_decimation = 1 # decimation by Morlet; reduces memory usage but removes the ability to keep true time!\n",
    "mean_bin_division = (resample_frequency / tfr_decimation) * 30 # division factor to bin samples into mean\n",
    "rolling_mean_samples = 3 # number of samples over which to calculate rolling mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morlet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file Cache/Subject01/S01_Feb02_256hz.fif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Layton\\AppData\\Local\\Temp\\ipykernel_21356\\4279266254.py:2: RuntimeWarning: This filename (Cache/Subject01/S01_Feb02_256hz.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif(fif_path, preload = True, verbose = None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isotrak not found\n",
      "    Range : 0 ... 1843573 =      0.000 ...  7201.457 secs\n",
      "Ready.\n",
      "Reading 0 ... 1843573  =      0.000 ...  7201.457 secs...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 02, 2022  05:41:39 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>51 sEEG, 1 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>256.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>128.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>S01_Feb02_256hz.fif</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>02:00:02 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Raw | S01_Feb02_256hz.fif, 52 x 1843574 (7201.5 s), ~731.5 MB, data loaded>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "raw = mne.io.read_raw_fif(fif_path, preload = True, verbose = None)\n",
    "\n",
    "# # Keep channels that have detected sleep events \n",
    "# # (also include the central sleep staging electrode)\n",
    "# events = pd.read_csv(events_path)\n",
    "# event_channels = events['channel'].unique().tolist()\n",
    "# keep_channels = event_channels + ['C4']\n",
    "# raw.pick_channels(ch_names = keep_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decimate to reduce memory usage\n",
    "raw.resample(resample_frequency)\n",
    "\n",
    "# Save timestamps for later\n",
    "timestamps = raw.times\n",
    "\n",
    "if tfr_decimation > 1:\n",
    "    timestamps = timestamps[::tfr_decimation]\n",
    "\n",
    "# Format Data for tfr_array_morlet()\n",
    "ts_array = raw.get_data(units = dict(seeg = 'uV', eeg = 'uV'))\n",
    "ts_array = ts_array[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  52 | elapsed:  1.7min remaining:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "freqs = np.arange(1, 26, 1)\n",
    "\n",
    "# Create time-frequency representation\n",
    "# using the Morlet Wavelet transform:\n",
    "tfr = tfr_array_morlet(ts_array, \n",
    "                       sfreq = raw.info['sfreq'],\n",
    "                       freqs = freqs, \n",
    "                       n_cycles = 6.0,\n",
    "                       zero_mean = False, \n",
    "                       use_fft = True, \n",
    "                       decim = tfr_decimation, \n",
    "                       output = 'power', \n",
    "                       n_jobs = -1, \n",
    "                       verbose = None)\n",
    "\n",
    "# Remove the dummy dimension (that was required\n",
    "# due to formatting expectations of MNE Morlet):\n",
    "tfr = np.squeeze(tfr)\n",
    "\n",
    "# Convert to Xarray as an intermediate step in\n",
    "# getting data into Pandas long (2d) format:\n",
    "tfr = xr.DataArray(tfr,\n",
    "                   dims = ('channel', 'frequency', 'seconds'),\n",
    "                   coords = {'channel' : raw.ch_names,\n",
    "                             'frequency' : freqs,\n",
    "                             'seconds' : timestamps})\n",
    "\n",
    "tfr = tfr.to_dataframe(name = 'power').reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Bin for Convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average 1 second values into 30s epoch values\n",
    "# (bin stop value is arbitrarily large, 1mil seconds is 277 hrs)\n",
    "# (alternative to the next two optional steps)\n",
    "bin_list = np.arange(0, 1000000, 30)\n",
    "\n",
    "tfr['epoch'] = pd.cut(tfr['seconds'], bins = bin_list, labels = False)\n",
    "tfr = tfr.groupby(['channel', 'frequency', 'epoch']).mean('power')\n",
    "tfr = tfr.reset_index()[['channel', 'frequency', 'epoch', 'power']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Normalize by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel-wise and frequency-wise normalization, \n",
    "# since each channel (each one has a different\n",
    "# baseline power magnitude) and frequency (1/f):\n",
    "tfr['meanpower'] = tfr.groupby(['channel', 'frequency'])['power'].transform('mean')\n",
    "tfr['log_meanpower'] = 10 * np.log10(tfr['meanpower'])\n",
    "\n",
    "tfr['logpower'] = 10 * np.log10(tfr['power'])\n",
    "tfr['logpower_mean'] = tfr.groupby(['channel', 'frequency'])['logpower'].transform('mean')\n",
    "\n",
    "tfr['logmpower_freq'] = tfr['logpower'] - tfr['log_meanpower']\n",
    "tfr['logpower_freq'] = tfr['logpower'] - tfr['logpower_mean']\n",
    "\n",
    "tfr.drop(columns = ['meanpower', 'log_meanpower', 'logpower_mean'], inplace = True)\n",
    "\n",
    "# Now calculate frequency-wise zscores from the log(power)\n",
    "tfr['lmpf_zscore'] = tfr.groupby(['channel', 'frequency'])['logmpower_freq'].transform(zscore)\n",
    "tfr['lpf_zscore'] = tfr.groupby(['channel', 'frequency'])['logpower_freq'].transform(zscore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, Smooth, and/or Decimate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this processing is different than the Morlet TFR computation for C4 in the `Transitions` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Layton\\AppData\\Local\\Temp\\ipykernel_21356\\2576674005.py:11: FutureWarning: Passing additional kwargs to RollingGroupby.mean has no impact on the result and is deprecated. This will raise a TypeError in a future version of pandas.\n",
      "  tfr = tfr.groupby(['channel', 'frequency']).rolling(window = rolling_mean_samples,\n"
     ]
    }
   ],
   "source": [
    "# # Group by chan, freq, and time bin; define the time bin using floor\n",
    "# # division of seconds by your desired decimation factor (optional step)\n",
    "# tfr['time_bin'] = tfr.groupby(['channel', 'frequency']).cumcount() + 1\n",
    "# tfr['time_bin'] = tfr['time_bin'] // mean_bin_division\n",
    "# tfr = tfr.groupby(['channel', 'frequency', 'time_bin']).mean()\n",
    "# tfr = tfr.reset_index()\n",
    "\n",
    "# Rolling mean to smooth TFR (optional step)\n",
    "saved_epochs = tfr['epoch']\n",
    "\n",
    "tfr = tfr.groupby(['channel', 'frequency']).rolling(window = rolling_mean_samples, \n",
    "                                                    min_periods = 1, \n",
    "                                                    center = True, \n",
    "                                                    win_type = 'gaussian').mean()\n",
    "tfr = tfr.reset_index()\n",
    "tfr.drop(columns = ['level_2'], inplace = True)\n",
    "\n",
    "tfr['epoch'] = saved_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr.to_csv(tfr_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(pd.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bceaa3bdda3825794b37c15d2000316b6f4a45a3d4f5e14660beed4f1d5f7638"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

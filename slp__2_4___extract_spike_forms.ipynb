{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "import hdf5storage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils__config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\Layton\\\\Sleep_083023'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'Cache/Subject01/Feb02/S01_Feb02_cnato__5___yemi_to_mat'\n",
    "dict_dir = 'Data/Subject01/S01_dictionary.xlsx'\n",
    "ld_dir = 'Cache/Subject01/Feb02/S01_logdensity_forms.csv'\n",
    "metric_dir = 'Cache/Subject01/Feb02/S01_spike_metrics.csv'\n",
    "out_dir = 'Cache/Subject01/Feb02/S01_spikeforms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAT_version = '7.3' # MAT file version (SciPy reads < 7.3, hdf5storage reads >= 7.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_dict = pd.read_excel(dict_dir)\n",
    "micro_dict = micro_dict[['number', 'laterality', 'region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [02:10<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame() # average waveform\n",
    "data_ld = pd.DataFrame() # log density waveform\n",
    "\n",
    "for channel in tqdm(os.listdir(root_dir)):\n",
    "\n",
    "    if MAT_version == '7.3':\n",
    "        raw_data = hdf5storage.loadmat(os.path.join(root_dir, channel))\n",
    "    else:\n",
    "        raw_data = io.loadmat(os.path.join(root_dir, channel))\n",
    "\n",
    "    chan_data = pd.DataFrame()\n",
    "    chan_data_ld = pd.DataFrame()\n",
    "\n",
    "    # Extract unit type and spike times\n",
    "    for unit in np.arange(0, len(raw_data['sp_types'])):\n",
    "\n",
    "        # Extract and format spike waveforms\n",
    "        unit_data = pd.DataFrame(raw_data['sp_waveforms'][unit][0])\n",
    "        unit_data = unit_data.reset_index()\n",
    "        unit_data['index'] = unit_data['index'] + 1\n",
    "        unit_data = pd.melt(unit_data, id_vars = 'index')\n",
    "        unit_data.columns = ['time_point', 'spike_id', 'amplitude']\n",
    "\n",
    "        # Define the bins for the histogram\n",
    "        x_bins = np.arange(1, 65)\n",
    "        y_bins = np.linspace(unit_data['amplitude'].min(), unit_data['amplitude'].max(), 101)  # 101 to get 100 bins\n",
    "\n",
    "        # Compute the 2D histogram for log density\n",
    "        histogram, xedges, yedges = np.histogram2d(unit_data['time_point'], unit_data['amplitude'], bins=(x_bins, y_bins))\n",
    "        log_density = np.log(histogram + 1)  # Add 1 to avoid log(0)\n",
    "\n",
    "        # Create a meshgrid for xedges and yedges\n",
    "        x, y = np.meshgrid(xedges[:-1], yedges[:-1], indexing='ij')\n",
    "\n",
    "        # Flatten the arrays and create a dataframe\n",
    "        unit_data_ld = pd.DataFrame({\n",
    "            'time_point': x.flatten(),\n",
    "            'amplitude': y.flatten(),\n",
    "            'log_density': log_density.flatten()})\n",
    "\n",
    "        # Average waveforms (optional step if using too much RAM)\n",
    "        unit_data = unit_data.groupby(['time_point']).mean().reset_index()\n",
    "\n",
    "        # Extract some meta-data\n",
    "        unit_type = raw_data['sp_types'][unit][0]\n",
    "\n",
    "        # Set meta-data & merge: AVERAGE WAVEFORM\n",
    "        unit_data['unit_type'] = unit_type\n",
    "        unit_data['unit_num'] = unit + 1\n",
    "        chan_data = pd.concat([chan_data, unit_data])\n",
    "\n",
    "        # Set meta-data & merge: LOG DENSITY WAVEFORM\n",
    "        unit_data_ld['unit_type'] = unit_type\n",
    "        unit_data_ld['unit_num'] = unit + 1\n",
    "        chan_data_ld = pd.concat([chan_data_ld, unit_data_ld])\n",
    "    \n",
    "    # Extract channel meta-data for AVERAGE WAVEFORM\n",
    "    chan_data['subject'] = channel.split('_')[0]\n",
    "    chan_data['channel'] = channel.split('_')[1]\n",
    "    chan_data['channel'] = chan_data['channel'].str.split('l', expand = True)[1]\n",
    "    chan_data['channel'] = chan_data['channel'].astype('int64') # to merge with micro_dict\n",
    "    chan_data['sign'] = channel.split('_')[2]\n",
    "    chan_data['sign'] = chan_data['sign'].str.split('.', expand = True)[0]\n",
    "    data = pd.concat([data, chan_data])\n",
    "\n",
    "    # Extract channel meta-data for LOG DENSITY WAVEFORM\n",
    "    chan_data_ld['subject'] = channel.split('_')[0]\n",
    "    chan_data_ld['channel'] = channel.split('_')[1]\n",
    "    chan_data_ld['channel'] = chan_data_ld['channel'].str.split('l', expand = True)[1]\n",
    "    chan_data_ld['channel'] = chan_data_ld['channel'].astype('int64') # to merge with micro_dict\n",
    "    chan_data_ld['sign'] = channel.split('_')[2]\n",
    "    chan_data_ld['sign'] = chan_data_ld['sign'].str.split('.', expand = True)[0]\n",
    "    data_ld = pd.concat([data_ld, chan_data_ld])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with dictionary meta-data\n",
    "data = data.merge(micro_dict, left_on = 'channel', right_on = 'number')\n",
    "data_ld = data_ld.merge(micro_dict, left_on = 'channel', right_on = 'number')\n",
    "\n",
    "# Account for the offset in unit number between Combinato and MATLAB\n",
    "# so that you can compare units between Combinato GUI and your analysis (optional)\n",
    "data['unit_num'] = data['unit_num'] - 1\n",
    "data_ld['unit_num'] = data_ld['unit_num'] - 1\n",
    "\n",
    "# Create a unique unit ID\n",
    "data['unit_id'] = data['subject'] + '_Ch' + data['channel'].astype('str') + '_' + data['sign'] + '_Unit' + data['unit_num'].astype('str')\n",
    "data_ld['unit_id'] = data_ld['subject'] + '_Ch' + data_ld['channel'].astype('str') + '_' + data_ld['sign'] + '_Unit' + data_ld['unit_num'].astype('str')\n",
    "\n",
    "# Rename laterality/region columns to specify that they apply to the unit\n",
    "data.rename(columns = {'laterality' : 'unit_laterality', 'region' : 'unit_region'}, inplace = True)\n",
    "data_ld.rename(columns = {'laterality' : 'unit_laterality', 'region' : 'unit_region'}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove artifactual units\n",
    "# (artifact = -1 | unassigned = 0 | MUA = 1 | SUA = 2)\n",
    "#data = data[data['unit_type'] == 2] # SUA only\n",
    "data = data[(data['unit_type'] != -1) & (data['unit_type'] != 0)] # SUA + MUA\n",
    "data_ld = data_ld[(data_ld['unit_type'] != -1) & (data_ld['unit_type'] != 0)] # SUA + MUA\n",
    "\n",
    "# Keep only units from CLA, AMY, ACC, or aINS\n",
    "data = data[(data['unit_region'] == 'CLA') | (data['unit_region'] == 'AMY') | \n",
    "            (data['unit_region'] == 'ACC') | (data['unit_region'] == 'aINS')]\n",
    "\n",
    "data_ld = data_ld[(data_ld['unit_region'] == 'CLA') | (data_ld['unit_region'] == 'AMY') | \n",
    "            (data_ld['unit_region'] == 'ACC') | (data_ld['unit_region'] == 'aINS')]\n",
    "\n",
    "# Keep only units selected by the quality control script\n",
    "spike_metrics = pd.read_csv(metric_dir)\n",
    "data = data[data['unit_id'].isin(spike_metrics['unit_id'])]\n",
    "data_ld = data_ld[data_ld['unit_id'].isin(spike_metrics['unit_id'])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Waveforms and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_vars = ['subject', 'unit_id', 'unit_region', \n",
    "                 'unit_laterality', 'sign', 'unit_type']\n",
    "\n",
    "# Find mean spike waveform (use if not using the optional group average in the loop cell)\n",
    "#data = data[(grouping_vars + ['time_point', 'amplitude'])]\n",
    "#data = data.groupby((grouping_vars + ['time_point'])).mean().reset_index()\n",
    "\n",
    "# Merge with spike metric meta-data\n",
    "spike_metrics = pd.read_csv(metric_dir)\n",
    "spike_metrics = spike_metrics[['unit_id', 'num_count', 'perc_isi_violations']]\n",
    "spike_metrics.columns = ['unit_id', 'count', 'isi']\n",
    "data = data.merge(spike_metrics, left_on = 'unit_id', right_on = 'unit_id')\n",
    "\n",
    "# Find the min/max/abs_max of mean waveforms\n",
    "min_max = data.groupby(grouping_vars).agg({'amplitude' : ['min', 'max']}).reset_index()\n",
    "min_max = min_max[['unit_id', 'amplitude']]\n",
    "min_max.columns = min_max.columns.droplevel()\n",
    "min_max.columns = ['unit_id', 'min', 'max']\n",
    "min_max['abs'] = min_max[['min', 'max']].abs().max(axis = 1)\n",
    "data = data.merge(min_max, on = 'unit_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "data.to_csv(out_dir, index = False)\n",
    "data_ld.to_csv(ld_dir, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
